from typing import Optional, Callable
import numpy as np
import torch
from torch.utils.data import Dataset
from PIL import Image
from decord import VideoReader, cpu
from torch import Tensor


class SlidingWindowVideoDataset(Dataset):
    """
    Flattens videos created by subsampling and sliding windowing.

    - A (raw) *video* is first subsampled by `frame_step` into a *clip*.
    - Then, *samples* are generated by by sliding windows of `num_frames` with `stride`.
    - The dataset is indexed by flattening all samples from all videos.
    """

    def __init__(
        self,
        videos: list[str | list[str]],
        num_frames: int = 16,
        stride: int = 4,
        frame_step: int = 2,
        start_frames: Optional[list[int]] = None,
        end_frames: Optional[list[int]] = None,
        transform: Optional[Callable] = None,
        validate_end: bool = True,
    ):
        """
        :param videos: List of video paths (either .mp4 paths or lists of .png paths)
        :param num_frames: Number of frames per window (e.g., 16)
        :param stride: Stride between windows (e.g., 4)
        :param frame_step: Frame sampling rate within video (e.g., 2 = every 2nd frame)
        :param start_frames: Per-video start frame in raw space (default: 0 for all)
        :param end_frames: Per-video end frame in raw space (default: video length for all)
        :param transform: Optional transform to apply to windows
        """
        assert start_frames is None or len(start_frames) == len(videos)
        assert end_frames is None or len(end_frames) == len(videos)
        assert stride < num_frames, "stride must be less than num_frames"

        self.videos = videos
        self.num_frames = num_frames
        self.stride = stride
        self.frame_step = frame_step
        self.start_frames = start_frames or [0] * len(videos)
        self.end_frames = end_frames or [self._get_video_length(v) for v in videos]
        self.transform = transform

        assert all(s >= 0 for s in self.start_frames), "start_frames[i] >= 0"
        if end_frames and validate_end:
            lens = [self._get_video_length(v) for v in videos]
            assert all(e <= l for e, l in zip(end_frames, lens)), "end exceeds length"  # noqa: E741

        # number of frames in a clip
        self.clip_lengths = [
            (end - start - 1) // frame_step + 1
            for start, end in zip(self.start_frames, self.end_frames)
        ]
        # number of samples per clip
        # assert all clip_length >= num_frames
        assert all(l >= num_frames for l in self.clip_lengths), (
            "clip_length < num_frames"
        )
        self._num_samples = [(l - num_frames) // stride + 1 for l in self.clip_lengths]  # noqa: E741
        self.flat_index: list[tuple[int, int]] = [
            (video_idx, sample_idx)
            for video_idx, num_samples in enumerate(self._num_samples)
            for sample_idx in range(num_samples)
        ]

    def __len__(self):
        return len(self.flat_index)

    def __getitem__(self, flat_idx: int) -> Tensor:
        video_idx, sample_idx = self.flat_index[flat_idx]
        sample = self._load_sample(video_idx, sample_idx)
        return self._apply_transform(sample)
    
    def _apply_transform(self, frames: Tensor) -> Tensor:
        if self.transform:
            frames = self.transform(frames)
        return frames

    def _get_video_length(self, video_path: str | list[str]) -> int:
        """Get video length."""
        if isinstance(video_path, str):  # .mp4
            return len(VideoReader(video_path, num_threads=-1, ctx=cpu(0)))
        else:  # list of .pngs
            return len(video_path)

    def _load_sample(self, video_idx: int, sample_idx: int) -> Tensor:
        """
        Load a single window from video.

        :param video_idx: Index of video in the list
        :param sample_idx: Sample index within the clip
        :return: Tensor of shape [T, H, W, C]
        """
        return self._load_frames(video_idx, sample_idx * self.stride, self.num_frames)

    def _load_clip(self, video_idx: int) -> Tensor:
        """
        Load entire subsampled clip from video.

        :param video_idx: Index of video in the list
        :return: Tensor of shape [T, H, W, C]
        """
        return self._load_frames(video_idx, 0, self.clip_lengths[video_idx])

    def _load_frames(
        self, video_idx: int, start_frame_offset: int, num_frames: int
    ) -> Tensor:
        """
        Load a single window from video.

        :param video_idx: Index of video in the list
        :param start_frame_offset: Offset (in frames) from clip start
        :param num_frames: Number of frames to load
        :return: Tensor of shape [T, H, W, C]
        """

        video_path = self.videos[video_idx]
        clip_start = self.start_frames[video_idx]

        raw_start = clip_start + start_frame_offset * self.frame_step
        raw_end = raw_start + num_frames * self.frame_step

        if isinstance(video_path, str):  # .mp4
            vr = VideoReader(video_path, num_threads=-1, ctx=cpu(0))
            raw_indices = np.arange(raw_start, raw_end, self.frame_step)
            frames = vr.get_batch(raw_indices).asnumpy()
            return torch.from_numpy(frames)
        else:  # list of .pngs
            frame_paths = video_path[raw_start : raw_end : self.frame_step]
            frames = [Image.open(path) for path in frame_paths]
            frames = [torch.from_numpy(np.array(f)) for f in frames]
            return torch.stack(frames)
