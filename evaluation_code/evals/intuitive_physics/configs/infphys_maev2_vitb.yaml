# TODO: this doesn't work, because the distilled version (vit-b) doesn't have the decoder. 
# We could try to use l1 on the latents, but it's not going to be as straightforward.
pretrain:
  enc_checkpoint_key: module
  pred_checkpoint_key: predictor
  model_name: pretrain_videomae_base_patch16_224
  patch_size: 16
  folder: /mnt/sdb1/checkpoint/intphys
  checkpoint: vit_b_k710_dl_from_giant.pth
  write_tag: videomaev2_b
  frames_per_clip: 16
data:
  batch_size: 1
  resolution: 224
  stride_sliding_window: 2
  use_bfloat16: true
  frames_per_clip: 16
  context_lengths: [2,4,6,8,10]
  frame_steps: 2
tag: videomaev2_b
nodes: 1
tasks_per_node: 6
eval_name: intuitive_physics
dataset: intphys
is_mae: true
mae_decoder_blocks: -1