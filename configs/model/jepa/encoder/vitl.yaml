# @package encoder

_target_: src.models.vision_transformer.VisionTransformer
patch_size: 16
embed_dim: 1024
depth: 24
num_heads: 16
mlp_ratio: 4
qkv_bias: true
norm_layer:
  _target_: torch.nn.LayerNorm
  _partial_: true
  eps: 1e-6

# def vit_large(patch_size=16, **kwargs):
#     model = VisionTransformer(
#         patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,
#         qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
#     return model